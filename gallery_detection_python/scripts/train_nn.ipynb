{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "import importlib\n",
    "from gallery_detection_models import models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    \"dataset_folder_path\": \"/media/lorenzo/SAM500/datasets/gallery_detection_dataset\",\n",
    "    \"n_samples\": None,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 32,\n",
    "    \"lr\": 0.00004,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"save_folder\": \"/media/lorenzo/SAM500/models/gallery-detection/\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalleryDetectionDataset(Dataset):\n",
    "    def __init__(self, index, n_desired_samples=None):\n",
    "        self.index = index\n",
    "        self.get_total_datapoints()\n",
    "        if n_desired_samples is None:\n",
    "            self.n_desired_samples = self.n_available_samples\n",
    "        else:\n",
    "            if self.n_available_samples > n_desired_samples:\n",
    "                self.n_desired_samples = n_desired_samples\n",
    "            else:\n",
    "                self.n_desired_samples = self.n_available_samples\n",
    "        self.set_n_samples_per_world()\n",
    "        self.load()\n",
    "\n",
    "    def get_total_datapoints(self):\n",
    "        data = self.index[\"data\"]\n",
    "        self.n_available_samples = 0\n",
    "        for world_name in data.keys():\n",
    "            self.n_available_samples += data[world_name][\"n_datapoints\"]\n",
    "\n",
    "    def set_n_samples_per_world(self):\n",
    "        self.n_samples_per_world = {}\n",
    "        for world_name in self.index[\"data\"].keys():\n",
    "            n_samples_in_world = self.index[\"data\"][world_name][\"n_datapoints\"]\n",
    "            self.n_samples_per_world[world_name] = int(\n",
    "                np.round(n_samples_in_world * self.n_desired_samples / self.n_available_samples)\n",
    "            )\n",
    "        self.final_n_datapoints = sum(\n",
    "            self.n_samples_per_world[k] for k in self.n_samples_per_world.keys()\n",
    "        )\n",
    "        print(self.n_samples_per_world)\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Allocating memory\")\n",
    "        self.images = torch.zeros(\n",
    "            (self.final_n_datapoints, 1, 16, self.index[\"info\"][\"image_width\"])\n",
    "        )\n",
    "        self.labels = torch.zeros((self.final_n_datapoints, 360))\n",
    "        global_index = 0\n",
    "        with tqdm(total=self.final_n_datapoints) as pbar:\n",
    "            for world_name in self.index[\"data\"].keys():\n",
    "                folder_name = self.index[\"data\"][world_name][\"images_folder\"]\n",
    "                samples_to_load = self.n_samples_per_world[world_name]\n",
    "                path_to_world_folder = os.path.join(\n",
    "                    self.index[\"info\"][\"path_to_dataset\"], folder_name\n",
    "                )\n",
    "                assert os.path.exists(path_to_world_folder)\n",
    "                raw_idxs = np.arange(0, self.index[\"data\"][world_name][\"n_datapoints\"])\n",
    "                np.random.shuffle(raw_idxs)\n",
    "                idxs = raw_idxs[:samples_to_load]\n",
    "                for idx in idxs:\n",
    "                    file_name = f\"{idx:010d}.npz\"\n",
    "                    path_to_file = os.path.join(path_to_world_folder, file_name)\n",
    "                    data = np.load(path_to_file)\n",
    "                    self.images[global_index, 0, :, :] = torch.tensor(data[\"image\"])\n",
    "                    self.labels[global_index] = torch.tensor(data[\"label\"])\n",
    "                    if torch.any(torch.isnan(self.labels[global_index])):\n",
    "                        print(\"There is a NaN in the raw label\")\n",
    "                    if torch.any(torch.isnan(self.labels[global_index])):\n",
    "                        print(\"There is a NaN after normalizing\")\n",
    "                    global_index += 1\n",
    "                    pbar.update(1)\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.final_n_datapoints\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, lbl = self.images[index], self.labels[index]\n",
    "        if torch.any(torch.isnan(lbl)):\n",
    "            print(\"Nan in __getitem__\")\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_018': 21777, 'env_007': 13324, 'env_010': 21551, 'env_020': 48033, 'env_017': 21721, 'env_002': 19991, 'env_015': 18613, 'env_008': 27916, 'env_003': 23698, 'env_016': 27254, 'env_005': 31794, 'env_013': 18436, 'env_019': 36708, 'env_004': 28563, 'env_011': 17309, 'env_014': 27147, 'env_006': 34637, 'env_009': 21455, 'env_012': 38653, 'env_001': 24794}\n",
      "Allocating memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523374/523374 [17:18<00:00, 504.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_folder_path = PARAMETERS[\"dataset_folder_path\"]\n",
    "path_to_index_file = os.path.join(dataset_folder_path, \"index.json\")\n",
    "assert os.path.exists(path_to_index_file)\n",
    "with open(path_to_index_file, \"r\") as f:\n",
    "    index = json.load(f)\n",
    "n_samples = PARAMETERS[\"n_samples\"]\n",
    "batch_size = PARAMETERS[\"batch_size\"]\n",
    "dataset = GalleryDetectionDataset(index, n_desired_samples=n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomErasing\n",
    "def augment(inpt: torch.Tensor):\n",
    "    \"\"\"The tensor should be of the shape B x C x H x W. Where:\n",
    "        - B: Batch size\n",
    "        - C: N channels\n",
    "        - H: Image height\n",
    "        - W: Image width\n",
    "    \"\"\"\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting parameters\")\n",
    "n_epochs = PARAMETERS[\"n_epochs\"]\n",
    "lr = PARAMETERS[\"lr\"]\n",
    "lr_decay = PARAMETERS[\"lr_decay\"]\n",
    "save_folder = PARAMETERS[\"save_folder\"]\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting neptune run\n",
      "https://app.neptune.ai/lcano/gallery-detection/e/GAL-117\n",
      "Saving at: /media/lorenzo/SAM500/models/gallery-detection/GalleryDetectorV3.v3.torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3156/2093568 [03:01<33:21:37, 17.41it/s]"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "print(\"Starting neptune run\")\n",
    "run = neptune.init_run(\n",
    "    project=\"lcano/gallery-detection\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYjcxZGU4OC00ZjVkLTRmMDAtYjBlMi0wYzkzNDQwOGJkNWUifQ==\",\n",
    "    capture_stderr=False,\n",
    "    capture_stdout=False,\n",
    ")  # your credentials\n",
    "model = models.GalleryDetectorV3(debug=False)\n",
    "#model.init_weights()\n",
    "model = model.type(torch.float)\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = ExponentialLR(optimizer, lr_decay)\n",
    "criterion = MSELoss(reduction=\"mean\")\n",
    "save_file_path = os.path.join(save_folder, f\"{model.__class__.__name__}.v3.torch\")\n",
    "print(f\"Saving at: {save_file_path}\")\n",
    "with tqdm(total = n_epochs * len(dataloader)) as pbar:\n",
    "    for n_epoch in range(n_epochs):\n",
    "        epoch_avg_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch_data in dataloader:\n",
    "            n_batches += 1\n",
    "            img, lbl = batch_data\n",
    "            if torch.any(torch.isnan(lbl)):\n",
    "                print(\"Break for NaN in label pre-cuda\")\n",
    "                break\n",
    "            img = img.to(\"cuda\").type(torch.float)\n",
    "            lbl = lbl.to(\"cuda\").type(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(img)\n",
    "            if torch.max(pred) == 0:\n",
    "                print(\"Break for collapse\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(img)):\n",
    "                print(\"Break for NaN in image\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(lbl)):\n",
    "                print(\"Break for NaN in label\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(pred)):\n",
    "                print(\"Break for NaN in prediction\")\n",
    "                break\n",
    "            loss = criterion(lbl, pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_avg_loss += loss.item()\n",
    "            run[\"train/loss\"].append(loss.item())\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            lr_scheduler.step()\n",
    "            epoch_avg_loss /= n_batches\n",
    "            if epoch_avg_loss < 0.03:\n",
    "                torch.save(model.to(\"cpu\").state_dict(), save_file_path)\n",
    "                model.to(\"cuda\")\n",
    "            fig = plt.figure()\n",
    "            axes1 = fig.add_axes([0, 0, 1, 1])\n",
    "            axes1.imshow(torch.clone(img[0][0]).detach().cpu().numpy())\n",
    "            axes2 = fig.add_axes([0, 1, 1, 1])\n",
    "            axes2.plot(torch.clone(pred[0].detach().cpu()).numpy())\n",
    "            axes2.plot(torch.clone(lbl[0].detach().cpu()).numpy())\n",
    "            run[\"predictions\"].append(fig)\n",
    "            plt.close()\n",
    "            continue\n",
    "        break\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
