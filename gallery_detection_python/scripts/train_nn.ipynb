{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "import importlib\n",
    "from gallery_detection_models import models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    \"dataset_folder_path\": [\"/media/lorenzo/SAM500/datasets/gallery_detection_dataset\",\"/media/lorenzo/SAM500/datasets/gallery_detection_smooth_straight\"],\n",
    "    \"n_samples\": None,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 64,\n",
    "    \"lr\": 0.00004,\n",
    "    \"lr_decay\": 0.99,\n",
    "    \"save_folder\": \"/media/lorenzo/SAM500/models/gallery-detection/\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalleryDetectionDataset(Dataset):\n",
    "    def __init__(self, index, n_desired_samples=None):\n",
    "        self.index = index\n",
    "        self.get_total_datapoints()\n",
    "        if n_desired_samples is None:\n",
    "            self.n_desired_samples = self.n_available_samples\n",
    "        else:\n",
    "            if self.n_available_samples > n_desired_samples:\n",
    "                self.n_desired_samples = n_desired_samples\n",
    "            else:\n",
    "                self.n_desired_samples = self.n_available_samples\n",
    "        self.set_n_samples_per_world()\n",
    "        self.load()\n",
    "\n",
    "    def get_total_datapoints(self):\n",
    "        data = self.index[\"data\"]\n",
    "        self.n_available_samples = 0\n",
    "        for world_name in data.keys():\n",
    "            self.n_available_samples += data[world_name][\"n_datapoints\"]\n",
    "\n",
    "    def set_n_samples_per_world(self):\n",
    "        self.n_samples_per_world = {}\n",
    "        for world_name in self.index[\"data\"].keys():\n",
    "            n_samples_in_world = self.index[\"data\"][world_name][\"n_datapoints\"]\n",
    "            self.n_samples_per_world[world_name] = int(\n",
    "                np.round(n_samples_in_world * self.n_desired_samples / self.n_available_samples)\n",
    "            )\n",
    "        self.final_n_datapoints = sum(\n",
    "            self.n_samples_per_world[k] for k in self.n_samples_per_world.keys()\n",
    "        )\n",
    "        print(self.n_samples_per_world)\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Allocating memory\")\n",
    "        self.images = torch.zeros(\n",
    "            (self.final_n_datapoints, 1, 16, self.index[\"info\"][\"image_width\"])\n",
    "        )\n",
    "        self.labels = torch.zeros((self.final_n_datapoints, 360))\n",
    "        global_index = 0\n",
    "        with tqdm(total=self.final_n_datapoints) as pbar:\n",
    "            for world_name in self.index[\"data\"].keys():\n",
    "                folder_name = self.index[\"data\"][world_name][\"images_folder\"]\n",
    "                samples_to_load = self.n_samples_per_world[world_name]\n",
    "                path_to_world_folder = os.path.join(\n",
    "                    self.index[\"info\"][\"path_to_dataset\"], folder_name\n",
    "                )\n",
    "                assert os.path.exists(path_to_world_folder)\n",
    "                raw_idxs = np.arange(0, self.index[\"data\"][world_name][\"n_datapoints\"])\n",
    "                np.random.shuffle(raw_idxs)\n",
    "                idxs = raw_idxs[:samples_to_load]\n",
    "                for idx in idxs:\n",
    "                    file_name = f\"{idx:010d}.npz\"\n",
    "                    path_to_file = os.path.join(path_to_world_folder, file_name)\n",
    "                    data = np.load(path_to_file)\n",
    "                    self.images[global_index, 0, :, :] = torch.tensor(data[\"image\"])\n",
    "                    self.labels[global_index] = torch.tensor(data[\"label\"])\n",
    "                    if torch.any(torch.isnan(self.labels[global_index])):\n",
    "                        print(\"There is a NaN in the raw label\")\n",
    "                    if torch.any(torch.isnan(self.labels[global_index])):\n",
    "                        print(\"There is a NaN after normalizing\")\n",
    "                    global_index += 1\n",
    "                    pbar.update(1)\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, lbl = self.images[index], self.labels[index]\n",
    "        if torch.any(torch.isnan(lbl)):\n",
    "            print(\"Nan in __getitem__\")\n",
    "        return img, lbl\n",
    "class CombinationDataset(Dataset):\n",
    "    def __init__(self, datasets):\n",
    "        for n_dataset, dataset in enumerate(datasets):\n",
    "            #assert isinstance(dataset, GalleryDetectionDataset)\n",
    "            if n_dataset == 0:\n",
    "                self.images = dataset.images\n",
    "                self.labels = dataset.labels\n",
    "                self.final_n_datapoints = dataset.final_n_datapoints\n",
    "            else:\n",
    "                self.images = torch.vstack((self.images, dataset.images))\n",
    "                self.labels = torch.vstack((self.labels, dataset.labels))\n",
    "                self.final_n_datapoints += dataset.final_n_datapoints\n",
    "            del dataset\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, lbl = self.images[index], self.labels[index]\n",
    "        if torch.any(torch.isnan(lbl)):\n",
    "            print(\"Nan in __getitem__\")\n",
    "        return img, lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/lorenzo/SAM500/datasets/gallery_detection_smooth_straight/index.json\n",
      "{'env_007': 7087, 'env_002': 18427, 'env_008': 12725, 'env_003': 11147, 'env_005': 5716, 'env_004': 11853, 'env_000': 9436, 'env_006': 9169, 'env_009': 9135, 'env_001': 11902}\n",
      "Allocating memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106597/106597 [03:19<00:00, 535.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "/media/lorenzo/SAM500/datasets/gallery_detection_dataset/index.json\n",
      "{'env_018': 21777, 'env_007': 13324, 'env_010': 21551, 'env_020': 48033, 'env_017': 21721, 'env_002': 19991, 'env_015': 18613, 'env_008': 27916, 'env_003': 23698, 'env_016': 27254, 'env_005': 31794, 'env_013': 18436, 'env_019': 36708, 'env_004': 28563, 'env_011': 17309, 'env_014': 27147, 'env_006': 34637, 'env_009': 21455, 'env_012': 38653, 'env_001': 24794}\n",
      "Allocating memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523374/523374 [12:07<00:00, 719.52it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "vstack() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6595/1035711894.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPARAMETERS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"batch_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msubdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGalleryDetectionDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_desired_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCombinationDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_6595/112268995.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datasets)\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_n_datapoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_n_datapoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_n_datapoints\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_n_datapoints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: vstack() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "subdatasets = []\n",
    "for dataset_folder_path in PARAMETERS[\"dataset_folder_path\"]:\n",
    "    path_to_index_file = os.path.join(dataset_folder_path, \"index.json\")\n",
    "    print(path_to_index_file)\n",
    "    assert os.path.exists(path_to_index_file)\n",
    "    with open(path_to_index_file, \"r\") as f:\n",
    "        index = json.load(f)\n",
    "    n_samples = PARAMETERS[\"n_samples\"]\n",
    "    batch_size = PARAMETERS[\"batch_size\"]\n",
    "    subdatasets.append(GalleryDetectionDataset(index, n_desired_samples=n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CombinationDataset(subdatasets)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomErasing\n",
    "\n",
    "def augment(x: torch.Tensor):\n",
    "    \"\"\"The tensor should be of the shape B x C x H x W. Where:\n",
    "    - B: Batch size\n",
    "    - C: N channels\n",
    "    - H: Image height\n",
    "    - W: Image width\n",
    "    \"\"\"\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    x = RandomErasing(p=0.5, scale=(0.005, 0.01), ratio=(0.3, 3.3))(x)\n",
    "    return x\n",
    " \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting parameters\")\n",
    "n_epochs = PARAMETERS[\"n_epochs\"]\n",
    "lr = PARAMETERS[\"lr\"]\n",
    "lr_decay = PARAMETERS[\"lr_decay\"]\n",
    "save_folder = PARAMETERS[\"save_folder\"]\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting neptune run\n",
      "https://app.neptune.ai/lcano/gallery-detection/e/GAL-129\n",
      "Saving at: /media/lorenzo/SAM500/models/gallery-detection/GalleryDetectorV2.v4.torch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315008/315008 [1:20:50<00:00, 64.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 218 operations to synchronize with Neptune. Do not kill this process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 218 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://app.neptune.ai/lcano/gallery-detection/e/GAL-129/metadata\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "print(\"Starting neptune run\")\n",
    "run = neptune.init_run(\n",
    "    project=\"lcano/gallery-detection\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYjcxZGU4OC00ZjVkLTRmMDAtYjBlMi0wYzkzNDQwOGJkNWUifQ==\",\n",
    "    capture_stderr=False,\n",
    "    capture_stdout=False,\n",
    ")  # your credentials\n",
    "model = models.GalleryDetectorV2()\n",
    "#model.init_weights()\n",
    "model = model.type(torch.float)\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = ExponentialLR(optimizer, lr_decay)\n",
    "criterion = MSELoss(reduction=\"mean\")\n",
    "save_file_path = os.path.join(save_folder, f\"{model.__class__.__name__}.v4.torch\")\n",
    "print(f\"Saving at: {save_file_path}\")\n",
    "with tqdm(total = n_epochs * len(dataloader)) as pbar:\n",
    "    for n_epoch in range(n_epochs):\n",
    "        epoch_avg_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch_data in dataloader:\n",
    "            n_batches += 1\n",
    "            img, lbl = batch_data\n",
    "            if torch.any(torch.isnan(lbl)):\n",
    "                print(\"Break for NaN in label pre-cuda\")\n",
    "                break\n",
    "            img = img.to(\"cuda\").type(torch.float)\n",
    "            if torch.rand(1).item()>0.5:\n",
    "                img = augment(img)\n",
    "            lbl = lbl.to(\"cuda\").type(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(img)\n",
    "            if torch.max(pred) == 0:\n",
    "                print(\"Break for collapse\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(img)):\n",
    "                print(\"Break for NaN in image\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(lbl)):\n",
    "                print(\"Break for NaN in label\")\n",
    "                break\n",
    "            if torch.any(torch.isnan(pred)):\n",
    "                print(\"Break for NaN in prediction\")\n",
    "                break\n",
    "            loss = criterion(lbl, pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_avg_loss += loss.item()\n",
    "            run[\"train/loss\"].append(loss.item())\n",
    "            pbar.update(1)\n",
    "        else:\n",
    "            lr_scheduler.step()\n",
    "            epoch_avg_loss /= n_batches\n",
    "            if epoch_avg_loss < 0.03:\n",
    "                torch.save(model.to(\"cpu\").state_dict(), save_file_path)\n",
    "                model.to(\"cuda\")\n",
    "            fig = plt.figure()\n",
    "            axes1 = fig.add_axes([0, 0, 1, 1])\n",
    "            axes1.imshow(torch.clone(img[0][0]).detach().cpu().numpy())\n",
    "            axes2 = fig.add_axes([0, 1, 1, 1])\n",
    "            axes2.plot(torch.clone(pred[0].detach().cpu()).numpy())\n",
    "            axes2.plot(torch.clone(lbl[0].detach().cpu()).numpy())\n",
    "            run[\"predictions\"].append(fig)\n",
    "            plt.close()\n",
    "            continue\n",
    "        break\n",
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
