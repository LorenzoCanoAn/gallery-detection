{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import MSELoss\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import neptune\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    \"dataset_folder_path\": \"/media/lorenzo/SAM500/datasets/temp_dataset\",\n",
    "    \"n_samples\": None,\n",
    "    \"batch_size\": 64,\n",
    "    \"n_epochs\": 512,\n",
    "    \"lr\": 0.00004,\n",
    "    \"lr_decay\": 0.999,\n",
    "    \"save_folder\": \"/media/lorenzo/SAM500/models/gallery-detection/\",\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalleryDetectorV3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv2d = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, [3, 5], padding=(0, 2), padding_mode=\"circular\", stride=(1, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, [3, 5], padding=(0, 2), padding_mode=\"circular\", stride=(1, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, [3, 5], padding=(0, 1), padding_mode=\"circular\", stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 16, [3, 3], padding=(0, 1), padding_mode=\"circular\", stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 8, [3, 3], padding=(0, 1), padding_mode=\"circular\", stride=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 1, [6, 3], padding=(0, 1), padding_mode=\"circular\", stride=(2, 1)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(254, 360),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.conv1d = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, 5, padding=2, padding_mode=\"circular\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 32, 3, padding=1, padding_mode=\"circular\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 16, 3, padding=1, padding_mode=\"circular\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 1, 3, padding=1, padding_mode=\"circular\"),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def is_2d(cls):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv2d(x)\n",
    "        # print(\"Conv2d\")\n",
    "        # print(torch.mean(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(\"Flatten\")\n",
    "        # print(torch.mean(x))\n",
    "        x = self.fc(x)\n",
    "        # print(\"Fc\")\n",
    "        # print(torch.mean(x))\n",
    "        x = torch.unsqueeze(x, 1)\n",
    "        # print(\"unsqz\")\n",
    "        # print(torch.mean(x))\n",
    "        x = self.conv1d(x)\n",
    "        # print(\"conv1d\")\n",
    "        # print(torch.mean(x))\n",
    "        x = torch.flatten(x, 1)\n",
    "        # print(\"flatten\")\n",
    "        # print(torch.mean(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GalleryDetectionDataset(Dataset):\n",
    "    def __init__(self, index, n_desired_samples=None):\n",
    "        self.index = index\n",
    "        self.get_total_datapoints()\n",
    "        if n_desired_samples is None:\n",
    "            self.n_desired_samples = self.n_available_samples\n",
    "        else:\n",
    "            if self.n_available_samples > n_desired_samples:\n",
    "                self.n_desired_samples = n_desired_samples\n",
    "            else:\n",
    "                self.n_desired_samples = self.n_available_samples\n",
    "        self.set_n_samples_per_world()\n",
    "        self.load()\n",
    "\n",
    "    def get_total_datapoints(self):\n",
    "        data = self.index[\"data\"]\n",
    "        self.n_available_samples = 0\n",
    "        for world_name in data.keys():\n",
    "            self.n_available_samples += data[world_name][\"n_datapoints\"]\n",
    "\n",
    "    def set_n_samples_per_world(self):\n",
    "        self.n_samples_per_world = {}\n",
    "        for world_name in self.index[\"data\"].keys():\n",
    "            n_samples_in_world = self.index[\"data\"][world_name][\"n_datapoints\"]\n",
    "            self.n_samples_per_world[world_name] = int(\n",
    "                np.round(n_samples_in_world * self.n_desired_samples / self.n_available_samples)\n",
    "            )\n",
    "        self.final_n_datapoints = sum(\n",
    "            self.n_samples_per_world[k] for k in self.n_samples_per_world.keys()\n",
    "        )\n",
    "        print(self.n_samples_per_world)\n",
    "\n",
    "    def load(self):\n",
    "        print(\"Allocating memory\")\n",
    "        self.images = torch.zeros(\n",
    "            (self.final_n_datapoints, 1, 16, self.index[\"info\"][\"image_width\"])\n",
    "        )\n",
    "        self.labels = torch.zeros((self.final_n_datapoints, 360))\n",
    "        global_index = 0\n",
    "        with tqdm(total=self.final_n_datapoints) as pbar:\n",
    "            for world_name in self.index[\"data\"].keys():\n",
    "                folder_name = self.index[\"data\"][world_name][\"images_folder\"]\n",
    "                samples_to_load = self.n_samples_per_world[world_name]\n",
    "                path_to_world_folder = os.path.join(\n",
    "                    self.index[\"info\"][\"path_to_dataset\"], folder_name\n",
    "                )\n",
    "                assert os.path.exists(path_to_world_folder)\n",
    "                raw_idxs = np.arange(0, self.index[\"data\"][world_name][\"n_datapoints\"])\n",
    "                np.random.shuffle(raw_idxs)\n",
    "                idxs = raw_idxs[:samples_to_load]\n",
    "                for idx in idxs:\n",
    "                    file_name = f\"{idx:010d}.npz\"\n",
    "                    path_to_file = os.path.join(path_to_world_folder, file_name)\n",
    "                    data = np.load(path_to_file)\n",
    "                    self.images[global_index, 0, :, :] = torch.tensor(data[\"image\"])\n",
    "                    self.labels[global_index] = torch.tensor(data[\"label\"])\n",
    "                    global_index += 1\n",
    "                    pbar.update(1)\n",
    "        print(\"Dataset loaded\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.final_n_datapoints\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting neptune run\n",
      "Getting parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting neptune run\")\n",
    "print(\"Getting parameters\")\n",
    "dataset_folder_path = PARAMETERS[\"dataset_folder_path\"]\n",
    "n_samples = PARAMETERS[\"n_samples\"]\n",
    "batch_size = PARAMETERS[\"batch_size\"]\n",
    "n_epochs = PARAMETERS[\"n_epochs\"]\n",
    "lr = PARAMETERS[\"lr\"]\n",
    "lr_decay = PARAMETERS[\"lr_decay\"]\n",
    "save_folder = PARAMETERS[\"save_folder\"]\n",
    "path_to_index_file = os.path.join(dataset_folder_path, \"index.json\")\n",
    "assert os.path.exists(path_to_index_file)\n",
    "with open(path_to_index_file, \"r\") as f:\n",
    "    index = json.load(f)\n",
    "os.makedirs(save_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_018': 21777, 'env_007': 13324, 'env_010': 21551, 'env_020': 48033, 'env_017': 21721, 'env_002': 19991, 'env_015': 18613, 'env_008': 27916, 'env_003': 23698, 'env_016': 27254, 'env_005': 31794, 'env_013': 18436, 'env_019': 36708, 'env_004': 28563, 'env_011': 17309, 'env_014': 27147, 'env_006': 34637, 'env_009': 21455, 'env_012': 38653, 'env_001': 24794}\n",
      "Allocating memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 523374/523374 [27:35<00:00, 316.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = GalleryDetectionDataset(index, n_desired_samples=n_samples)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GalleryDetectorV3()\n",
    "model = model.type(torch.float)\n",
    "model = model.to(\"cuda\")\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "lr_scheduler = ExponentialLR(optimizer, lr_decay)\n",
    "criterion = MSELoss(reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8503/85116997.py:1: NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n",
      "  run = neptune.init_run(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://app.neptune.ai/lcano/gallery-detection/e/GAL-60\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=\"lcano/gallery-detection\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiYjcxZGU4OC00ZjVkLTRmMDAtYjBlMi0wYzkzNDQwOGJkNWUifQ==\",\n",
    "    capture_stderr=False,\n",
    "    capture_stdout=False,\n",
    ")  # your credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4187136/4187136 [46:01:05<00:00, 25.27it/s]   \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total = n_epochs * len(dataloader)) as pbar:\n",
    "    for n_epoch in range(n_epochs):\n",
    "        epoch_avg_loss = 0\n",
    "        n_batches = 0\n",
    "        for batch_data in dataloader:\n",
    "            n_batches += 1\n",
    "            img, lbl = batch_data\n",
    "            img = img.to(\"cuda\").type(torch.float)\n",
    "            lbl = lbl.to(\"cuda\").type(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(img)\n",
    "            loss = criterion(lbl, pred)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_avg_loss += loss.item()\n",
    "            run[\"train/loss\"].append(loss.item())\n",
    "            pbar.update(1)\n",
    "        lr_scheduler.step()\n",
    "        epoch_avg_loss /= n_batches\n",
    "        if epoch_avg_loss < 0.002:\n",
    "            save_file_path = os.path.join(save_folder, f\"{model.__class__.__name__}.torch\")\n",
    "            torch.save(model.state_dict(), save_file_path)\n",
    "        fig = plt.figure()\n",
    "        axes1 = fig.add_axes([0, 0, 1, 1])\n",
    "        axes1.imshow(torch.clone(img[0][0]).detach().cpu().numpy())\n",
    "        axes2 = fig.add_axes([0, 1, 1, 1])\n",
    "        axes2.plot(torch.clone(pred[0].detach().cpu()).numpy())\n",
    "        axes2.plot(torch.clone(lbl[0].detach().cpu()).numpy())\n",
    "        run[\"predictions\"].append(fig)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "train_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
